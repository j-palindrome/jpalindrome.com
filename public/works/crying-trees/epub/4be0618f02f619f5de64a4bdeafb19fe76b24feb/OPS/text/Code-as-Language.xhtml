<?xml version="1.0" encoding="UTF-8" standalone="no"?>
        <html xmlns="http://www.w3.org/1999/xhtml"
          xmlns:epub="http://www.idpf.org/2007/ops"
          xmlns:ibooks="http://vocabulary.itunes.apple.com/rdf/ibooks/vocabulary-extensions-1.0"
          epub:prefix="ibooks: http://vocabulary.itunes.apple.com/rdf/ibooks/vocabulary-extensions-1.0">
        <head>
          <title></title>
          <meta http-equiv="default-style" content="text/html charset=utf-8"/>
          <meta name="robots" content="index,follow"/>
          <link rel="stylesheet" type="text/css" href="../stylesheets/application.css"/>
        </head>
        <body>
      
<!-- START: section:chapter#code-as-language; _markdown/Code-as-Language.md:4 -->
<section id="code-as-language" epub:type="bodymatter chapter" class="bodymatter chapter"><h1>Code as language</h1>

    
<!-- START: video:video-inline#digitizing1; -->

      <section class="video video--16x9 figure__large figure__large__controls--no figure__inline">
        <video id="digitizing1" autoplay="autoplay" loop="loop" muted="muted" webkit-playsinline="webkit-playsinline" playsinline="playsinline">
          <source src="../media/vid4.mp4" type="video/mp4"/>
          
          <p class="media__fallback media__fallback__video media__fallback--text">Your device does not support the HTML5 video API.</p>
        </video>
        
      </section>
    
<!-- END: video:video-inline#digitizing1; -->
<p>Coding frameworks themselves contain inherent assumptions, ideologies, and configurations. The content of code becomes especially relevant when producing digital content that is embedded into HTML, JavaScript, and CSS: the natural language of a digital essay is embedded within the constructed language of coding directives.</p>
<h2>Prescriptive vs. descriptive</h2>
<p>Any language has prescriptive and descriptive usages: the prescriptive being dictionary definitions, and the descriptive being actual speakers of the language. The prescriptive and descriptive are in constant dialogue, with standards changing in response to usage, and usage often governed by standards. JavaScript is a language accompanied by a standards organization: the <a href="https://ecma-international.org/wp-content/uploads/ECMA-262_15th_edition_june_2024.pdf">ECMA</a>. The JavaScript standards are something like the Spanish &quot;<a href="https://www.rae.es">Real Academie Española</a>,&quot; which produces dictionaries, standardizes grammar conventions, and more. While English has no managing entity, the Oxford English Dictionary tends to be regarded as a lexical standard.</p>
<p>The OED has a tradition of adding words: for example, &quot;<a href="https://www.oed.com/dictionary/selfie_n?tl=true">selfie</a>&quot; in 2013. This practice is an example of how descriptive trends gradually become new prescriptive standards: as usage multiplies, language authorities take notice. And language usage is increasingly shaped by the digital sphere—as the OED's dictionary definition requires a &quot;selfie&quot; to be &quot;shared via social media.&quot;</p>
<p>JavaScript has prescriptive and descriptive aspects as well, although these conventions are more technologically embedded than traditional language use: the ECMA standards evolve in response to developer needs, integrating extensions into the core language. Importantly, corporations (read: Google, Apple, Mozilla, and Microsoft) have stakes in JavaScript development, as each browser implements its own JavaScript interpreter. The history of standards development is rich with browser-wars, as organizations vied for incremental or massive upgrades (<a href="https://www.w3schools.com/js/js_history.asp#:~:text=JavaScript%20was%20invented%20by%20Brendan,Mozilla's%20latest%20version%20was%201.8.">W3Schools</a>).</p>
<h2>Rhizomatic and arborescent</h2>
<p>The tension between prescriptive and descriptive connects to the innovation in knowledge systems that the Internet constitutes. I draw from Deleuze and Guattari's concept of the <em>rhizome</em> to illuminate these contradictions.</p>
<p>An older method of indexing stems from taxonomies: for example, the classic <a href="https://www.w3schools.com/js/js_history.asp#:~:text=JavaScript%20was%20invented%20by%20Brendan,Mozilla's%20latest%20version%20was%201.8.">Dewey Decimal</a> system. Dewey considered a preconceived categorization of human knowledge, in which contents (most often, print books) grouped themselves into smaller and smaller subsections of that numbering. The index is based on large to small, branching outwards. Deleuze and Guattari refer to this structure as <em>arborescence</em>: a root system which branches outwards from an origin. The arborescent knowledge structure is prescriptive: it proceeds by definitions, a priori.</p>
<p>In contrast, a rhizome is small to large: it is a network. Modeled on the growth structures of grass and underground burrows, the rhizome is flat and pluralistic. Any part of a rhizome can connect to any other part, any node in the network connects to any other. <a href="https://en.wikipedia.org/wiki/Main_Page">Wikipedia</a> exemplifies a rhizomatic knowledge structure: rather than a line of books visible to the browser, one starts from a search, proceeding in structure to other networks of knowledge. The rhizome is descriptive: structure emerges from connection and possibility.</p>
<h2>Openness</h2>
<p>The rhizome of the Internet offers possibilities for knowledge to proliferate in a generative way: publishers are no longer bound by issues or release dates, and perhaps have more open-access channels to create networks of knowledge. This structure creates a sort of digital utopianism, a feeling of radical openness.</p>
<p>This radical openness has inspired many artists and thinkers, including Legacy Russell in her recent book <em>Glitch Feminism</em>:</p>
<blockquote>
<p>For my body, then, subversion came via digital remix, searching for those sites of experimentation where I could explore my true self, open and ready to be read by those who spoke my language. Online, I sought to become a fugitive from the mainstream, unwilling to accept its limited definition of bodies like my own. (<em>Glitch Feminism</em>, &quot;Introduction&quot;)</p>
</blockquote>
<p>Russell is excited by the possibilities of &quot;language&quot; afforded within the Internet, situating it as a primarily descriptive practice: a &quot;remix&quot; for playing with, and bucking, conventions. The power of remix is located in the networked capabilities of the Internet, the capacity to create connections via a rhizomatic structure that avoids contemporary channels such as the &quot;mainstream.&quot; Russell looks to the bottom-up, descriptive nature of Internet organization, the emergent capabilities of networks and the proliferation of selves that the Internet affords.</p>
<p>This attitude encapsulates the potential of the Internet to literally generate new forms of language—the language of code serving as a platform for human languages that are more &quot;true,&quot; that are &quot;fugitive.&quot; The Internet becomes a dual space of anonymity and truth, where one avoids the searching gaze of the mainstream and can instead be viewed as one wishes to present oneself. Russell is hopeful that finally, &quot;the Internet still remains a vessel through which a 'becoming' can realize itself&quot; (<em>Glitch Feminism</em>, &quot;Introduction&quot;).</p>
<h2>Extraction</h2>
<p>The Web is subject to a contradictory discourse: search engines like Google have achieved a monopoly by understanding the very innovations that make the web a pluralistic, utopian idea.</p>
<p>Google realized quickly that the old ways of searching keywords would not work on the Internet. Most keyword-based search tools use a vector-space model which organizes text according to similarity to the search terms, similar to a frequency of word occurrence (Brin and Page, &quot;<a href="http://infolab.stanford.edu/~backrub/google.html">The Anatomy of a Large-Scale Hypertextual Web Search Engine</a>&quot;). However, this method fails to match relevant pages on the unfiltered Internet, as &quot;junk results&quot; can fill up a search by simply matching short search queries exactly. Periodicals and books are assisted by their standard length, but webpages have no such consistency. Additionally, keyword searches can be hacked by standard webpage practices. Google's goal was to resist easy SEO (&quot;search engine optimization&quot;)—so that webpages couldn't hack its algorithms by embedding pervasive keywords or metadata into pages.</p>
<p>The alternative was for Google to understand the Internet through the lenses of the rhizome and fugitivity that Russell and Deleuze &amp; Guattari advocate. Google's success stemmed primarily from its PageRank algorithm, which capitalized upon the &quot;citation (link) graph of the web [...] an important resource that ha[d] largely gone unused in existing web search engines&quot; (&quot;Anatomy,&quot; 2.1). Pages no longer needed to simply match search keywords: Google's crawlers marked their place within the emergent network of the Internet, turning the rhizome of hyperlinks back into a hierarchical search sort.</p>
<p>Google no longer needed pages to even conform to standard HTML: it built a custom parser that accommodated the inconsistencies inherent in webpages—&quot;typos in HTML tags to kilobytes of zeros in the middle of a tag, non-ASCII characters, HTML tags nested hundreds deep, and a great variety of other errors that challenge anyone's imagination to come up with equally creative ones&quot; (&quot;Anatomy,&quot; 4.4). Instead, by tracing the network of descriptive hyperlinks between pages, Google condensed its information to anchor text and link structure: links, and the commentary on the links. Pages could be opaque and obfuscated: Google would illuminate them.</p>
<h2>Tensions Between Standards</h2>
<p>I bring up the Google search algorithm because it reveals the complexities inherent in digital utopianism, and efforts to use the Internet as a creative space. The anonymity, self-invention, and emergent networks of the Internet lent themselves just as well to the monopoly that Google became, turning into an increasingly standardized Internet in which certain players dominate the search results, and fringe publications have a difficult time ranking high in the lists, getting discovered. Given the challenges and opportunities of digital publishing, how does one function as radical digital organizations within this situation?</p>
<p>One method is by creating unique platforms that extend the functionality of the standard webpage. This involves &quot;hacking&quot; default browser behavior using CSS/JavaScript, and potentially creating alternatives to standard HTML that enable more diverse skillsets to interface with the webpage. To adopt a materialist view of the Web, to build a custom site is to take control of the means of production. This falls short of a Marxist seizure, of a course—the networked, algorithmic Internet falls outside of any political rule, enabling and enacting multinational, corporate power structures—but custom web design is necessary for formal innovation.</p>
<p>Anything innovating the Web is by nature a translation project, as cutting-edge JavaScript has to be translated back to the legacy, more commonly adopted versions. Often, these newer frameworks consider DX, or developer experience, inventing shorthands that make writing code clearer and more fit to the developer's purpose. I now investigate the frameworks which Triple Canopy developed to serve its purposes as a radical digital organization.</p>
</section>
<!-- END: section:#code-as-language -->
<script type="application/javascript" src="../javascripts/application.js"></script><script type="application/javascript">window.bber = window.bber || {}; window.bber.env = 'reader';</script></body></html>